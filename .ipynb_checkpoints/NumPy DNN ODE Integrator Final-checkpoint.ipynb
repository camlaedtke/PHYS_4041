{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.utils import resample\n",
    "from src.optimizers import SGD, Adam\n",
    "from src.utils import generate_batches\n",
    "from src.losses import mean_squared_error\n",
    "from src.activations import tanh, d_tanh\n",
    "from src.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ode_generators import generate_damped_pendulum_solution\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, x, dxdt = generate_damped_pendulum_solution(one_step=False, α=0.1, β=8.91, t_end=20, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"State Trajectory\", fontsize=22)\n",
    "plt.plot(t, x, \"-\", label=\"x\")\n",
    "plt.plot(t, dxdt, \"-\", label=\"dxdt\")\n",
    "plt.xlabel(\"t\", fontsize=20)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Phase Space\", fontsize=22)\n",
    "plt.plot(x, dxdt, \"-\")\n",
    "plt.xlabel(\"x\", fontsize=20)\n",
    "plt.ylabel(\"dxdt\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_damped_pendulum_solution(N_SAMPLES=25000, α=0.1, β=8.91, Δ=0.1, one_step=True, noise=0.05)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepResNet:\n",
    "    \n",
    "    def __init__(self, nn_architecture, loss_function, optimizer, learning_schedule=None, track_gradients=False):\n",
    "        self.nn_architecture = nn_architecture\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_schedule = learning_schedule\n",
    "\n",
    "        # initiation of lists storing the history of metrics calculated during the learning process \n",
    "        self.train_cost_history = []\n",
    "        self.valid_cost_history = []\n",
    "        self.track_gradients = track_gradients\n",
    "        self.grads_history = {}\n",
    "        # variables for learning rate scheduling\n",
    "        self.performance_metric = 1e12\n",
    "        self.wait = 0\n",
    "        \n",
    "        \n",
    "    def init_layers(self, seed=99):\n",
    "        # random seed initiation\n",
    "        np.random.seed(seed)\n",
    "        # number of layers in our neural network\n",
    "        number_of_layers = len(self.nn_architecture)\n",
    "        # parameters storage initiation\n",
    "        params_values = {}\n",
    "        # iteration over network layers\n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            # we number network layers from 1\n",
    "            layer_idx = idx + 1\n",
    "            # extracting the number of units in layers\n",
    "            layer_input_size = layer[\"input_dim\"]\n",
    "            layer_output_size = layer[\"output_dim\"]\n",
    "            # initiating the values of the W matrix and vector b for subsequent layers\n",
    "            params_values['W'+str(layer_idx)] = np.random.randn(layer_output_size, layer_input_size) * 1\n",
    "            params_values['b'+str(layer_idx)] = np.zeros((layer_output_size, 1))\n",
    "\n",
    "        return params_values\n",
    "    \n",
    "    \n",
    "    def init_grads_history(self):\n",
    "        \n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.nn_architecture))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            \n",
    "            self.grads_history[\"dW\" + str(layer_idx_curr)] = []\n",
    "            self.grads_history[\"db\" + str(layer_idx_curr)] = []\n",
    "\n",
    "    \n",
    "    def forward_propagation(self, X, params_values):\n",
    "        # creating a temporary memory to store the information needed for a backward step\n",
    "        memory = {}\n",
    "        # X vector is the activation for layer 0 \n",
    "        A_curr = X\n",
    "\n",
    "        # iteration over network layers\n",
    "        for idx, layer in enumerate(self.nn_architecture):\n",
    "            # we number network layers from 1\n",
    "            layer_idx = idx + 1\n",
    "            \n",
    "            # transfer the activation from the previous iteration or add the residual if \"skip\"\n",
    "            if layer[\"activation\"] is \"skip\":\n",
    "                A_prev = A_curr + X\n",
    "            else:\n",
    "                A_prev = A_curr\n",
    "                \n",
    "            # extraction of W for the current layer\n",
    "            W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "            # extraction of b for the current layer\n",
    "            b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "            # calculation of the input value for the activation function\n",
    "            Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "            \n",
    "            # calculation of activation for the current layer ()\n",
    "            if layer[\"activation\"] is not \"skip\":\n",
    "                A_curr = tanh(Z_curr)\n",
    "            else:\n",
    "                A_curr = Z_curr\n",
    "            \n",
    "            # saving calculated values in the memory\n",
    "            if layer[\"activation\"] is not \"skip\":\n",
    "                memory[\"A\" + str(idx)] = A_prev\n",
    "                \n",
    "            memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "\n",
    "        # return of prediction vector and a dictionary containing intermediate values\n",
    "        return A_curr, memory\n",
    "    \n",
    "    \n",
    "    def backward_propagation(self, Y_hat, Y, memory, params_values):\n",
    "        grads_values = {}\n",
    "\n",
    "        # number of examples\n",
    "        m = Y.shape[1]\n",
    "        # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "        Y = Y.reshape(Y_hat.shape)\n",
    "\n",
    "        # initiation of gradient descent algorithm (Derivative of mse loss function)\n",
    "        dCost = 2*(Y_hat - Y)\n",
    "        \n",
    "        dA_prev = dCost\n",
    "\n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.nn_architecture))):\n",
    "            # we number network layers from 1\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            \n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            # extraction of the activation function for the current layer\n",
    "            if layer[\"activation\"] is not \"skip\":\n",
    "                A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "                \n",
    "            Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "\n",
    "            W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "            b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "            \n",
    "            if layer[\"activation\"] == \"skip\":\n",
    "                # Last layer has no activation function, so slightly different back prop\n",
    "                dZ_curr = dCost \n",
    "                # derivative of the matrix W\n",
    "                dW_curr = np.dot(dCost, Z_curr.T) / m\n",
    "                # derivative of the vector b\n",
    "                db_curr = np.sum(dCost, axis=1, keepdims=True) / m\n",
    "                # derivative of the matrix A_prev\n",
    "                dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "            else:\n",
    "                # calculation of the activation function derivative \n",
    "                dZ_curr = dA_curr * d_tanh(Z_curr)\n",
    "                # derivative of the matrix W\n",
    "                dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "                # derivative of the vector b\n",
    "                db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "                # derivative of the matrix A_prev\n",
    "                dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "                \n",
    "            grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr.astype(np.float32)\n",
    "            grads_values[\"db\" + str(layer_idx_curr)] = db_curr.astype(np.float32)\n",
    "\n",
    "        return grads_values\n",
    "    \n",
    "    \n",
    "    def log_gradients(self, grads_values):\n",
    "        \n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.nn_architecture))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            \n",
    "            dW = grads_values[\"dW\" + str(layer_idx_curr)]\n",
    "            db = grads_values[\"db\" + str(layer_idx_curr)]\n",
    "            \n",
    "            self.grads_history[\"dW\" + str(layer_idx_curr)].append(np.mean(dW, axis=0))\n",
    "            self.grads_history[\"db\" + str(layer_idx_curr)].append(np.mean(db, axis=0))\n",
    "                            \n",
    "            \n",
    "    def train(self, X_train, Y_train, epochs, X_valid, Y_valid, batch_size=10):\n",
    "        \n",
    "        # initiation of neural net parameters\n",
    "        params_values = self.init_layers()\n",
    "        self.init_grads_history()\n",
    "        \n",
    "        X_valid = np.transpose(X_valid)\n",
    "        Y_valid = np.transpose(Y_valid)\n",
    "            \n",
    "        # performing calculations for subsequent iterations\n",
    "        for i in range(epochs):\n",
    "            # Reshuffle the data \n",
    "            X_train_, Y_train_ = resample(X_train, Y_train, n_samples=None)\n",
    "            # then transpose back into \"correct\" shape \n",
    "            X_train_ = np.transpose(X_train_)\n",
    "            Y_train_ = np.transpose(Y_train_)\n",
    "            \n",
    "            epoch_train_cost = []\n",
    "            epoch_valid_cost = []\n",
    "\n",
    "            for idx, (X_batch, Y_batch) in enumerate(generate_batches(X_train_, Y_train_, batch_size)):\n",
    "                \n",
    "                # step forward\n",
    "                Y_hat, cache = self.forward_propagation(X_batch, params_values)\n",
    "\n",
    "                # calculating metrics and saving them in history\n",
    "                cost = self.loss_function(Y_hat, Y_batch)\n",
    "                epoch_train_cost.append(cost)\n",
    "\n",
    "                # step backward - calculating gradient\n",
    "                grads_values = self.backward_propagation(Y_hat, Y_batch, cache, params_values)\n",
    "                \n",
    "                if self.track_gradients:\n",
    "                    self.log_gradients(grads_values)\n",
    "            \n",
    "                # updating model state\n",
    "                params_values = self.optimizer.update(params_values, grads_values, self.nn_architecture)\n",
    "                \n",
    "            # step forward\n",
    "            Y_hat, cache = self.forward_propagation(X_valid, params_values)\n",
    "\n",
    "            # calculating metrics and saving them in history\n",
    "            epoch_valid_cost = self.loss_function(Y_hat, Y_valid)               \n",
    "            epoch_mean_train_cost = np.mean(epoch_train_cost)\n",
    "            self.train_cost_history.append(epoch_mean_train_cost)\n",
    "            self.valid_cost_history.append(epoch_valid_cost)\n",
    "            \n",
    "            if self.learning_schedule is not None:\n",
    "                self.learning_schedule.update(epoch_valid_cost, self.optimizer)\n",
    "\n",
    "            print(\"\\r Epoch: {:05} - Train cost: {:.7f} - Valid cost: {:.7f}\".format(i, \n",
    "                                                    epoch_mean_train_cost, epoch_valid_cost), end='')\n",
    "\n",
    "        return params_values\n",
    "    \n",
    "    \n",
    "    def predict(self, X, params_values):\n",
    "        y , _ = self.forward_propagation(np.expand_dims(X, axis=0).T, params_values)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing NumPy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_ARCHITECTURE = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 30,\"activation\": \"tanh\"},\n",
    "    {\"input_dim\": 30,\"output_dim\": 30,\"activation\": \"tanh\"},\n",
    "    {\"input_dim\": 30,\"output_dim\": 2, \"activation\": \"tanh\"},\n",
    "    {\"input_dim\": 2, \"output_dim\": 2, \"activation\": \"skip\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneStepResNet(\n",
    "    nn_architecture = NN_ARCHITECTURE,\n",
    "    loss_function = mean_squared_error,\n",
    "    optimizer = Adam(lr = 0.001),\n",
    "    learning_schedule = ReduceLROnPlateau(patience=100, factor=0.5, min_lr=1e-5),\n",
    "    track_gradients = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_values = model.train(\n",
    "    X_train = X_train,\n",
    "    Y_train = y_train, \n",
    "    epochs = 500, \n",
    "    X_valid = X_valid,\n",
    "    Y_valid = y_valid,\n",
    "    batch_size = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model): \n",
    "    train_loss = model.train_cost_history\n",
    "    valid_loss = model.valid_cost_history\n",
    "    epochs = range(len(train_loss))\n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.plot(epochs, train_loss, 'b', alpha=0.7, label=\"Training\")\n",
    "    plt.plot(epochs, valid_loss, 'r', alpha=0.7, label=\"Validation\")\n",
    "    plt.title('Training and Validation Loss',fontsize=22)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim([0,0.1])\n",
    "    plt.xlabel(\"Epoch\", fontsize=20)\n",
    "    plt.legend(prop={\"size\":20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: get cool animation by calling `predict_solution` many times during training and clearning output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_solution(Δ=0.1, t_end=20):\n",
    "\n",
    "    t_steps = np.arange(0,t_end, Δ)\n",
    "    X_pred = np.zeros((len(t_steps), 2))\n",
    "\n",
    "    x_0 = np.array([-1.193, -3.876])\n",
    "    x_Δ = np.array([-1.193, -3.876])\n",
    "    \n",
    "    for i in range(0, len(t_steps)):\n",
    "        x_0 = x_Δ\n",
    "        x_Δ = model.predict(x_0, params_values)\n",
    "        x_Δ = np.squeeze(x_Δ.T)\n",
    "        X_pred[i] = x_Δ   \n",
    "        \n",
    "    t_steps = t_steps + Δ\n",
    "    return t_steps, X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(t_actual, x_actual, t_pred, x_pred, t_end):\n",
    "    Nt = t_actual.shape[0]\n",
    "    errors = []\n",
    "    for i in range(0,len(t_pred)):\n",
    "        t_idx = int((Nt/t_end)*t_pred[i] - 1)\n",
    "        x_true = x_actual[t_idx]\n",
    "        x_est = x_pred[i]\n",
    "        err = np.abs((x_true - x_est)/x_true)*100\n",
    "        #print(\"x true = {:.4f}  x est = {:.4f}  error = {:.4f}\".format(x_true, x_est, err))\n",
    "        errors.append(err)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δ=0.1\n",
    "t_end=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps, X_pred = predict_solution(Δ=Δ, t_end=t_end)  \n",
    "t, x, dxdt = generate_damped_pendulum_solution(one_step=False, α=0.1, β=8.91, Δ=Δ, t_end=t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error = get_error(t_actual=t, x_actual=x, t_pred=t_steps, x_pred=X_pred[:,0], t_end=t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOUR1 = (0.99, 0.72, 0.01, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title(\"Position\", fontsize=22)\n",
    "plt.plot(t, x, \"-\", c=\"r\", linewidth=3, label=\"x(t)\")\n",
    "plt.plot(t_steps, X_pred[:,0], \".\", c=\"b\", markersize=10, label=\"x(t) Predicted\")\n",
    "t_idx = [int(tx) for tx in (len(t)/t_end)*t_steps-1]\n",
    "plt.fill_between(t_steps, X_pred[:,0], x[t_idx], color=COLOUR1, alpha=0.5)\n",
    "plt.legend(prop={\"size\":16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title(\"Velocity\", fontsize=22)\n",
    "plt.plot(t, dxdt, \"-\", c=\"r\", linewidth=3, label=\"dx/dt (t)\")\n",
    "plt.plot(t_steps, X_pred[:,1],\".\", c=\"b\", markersize=10, label=\"dx/dt (t) Predicted\")\n",
    "t_idx = [int(tx) for tx in (len(t)/t_end)*t_steps-1]\n",
    "plt.fill_between(t_steps, X_pred[:,1], dxdt[t_idx], color=COLOUR1, alpha=0.5)\n",
    "plt.legend(prop={\"size\":16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title(\"Phase Space\", fontsize=22)\n",
    "plt.plot(x, dxdt, \"-\", c=\"r\", linewidth=2, label=\"Reference\")\n",
    "plt.plot(X_pred[:,0], X_pred[:,1], \":.\", c=\"b\", markersize=10, label=\"Approximation\")\n",
    "plt.legend(prop={\"size\":16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title(\"Percent Error\", fontsize=22)\n",
    "plt.plot(t_steps, error, \":.\", linewidth=2, c=\"r\", markerfacecolor='blue', markersize=12)\n",
    "plt.ylabel(\"% Error\", fontsize=20)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
